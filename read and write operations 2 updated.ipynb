{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eef4cbb-25c7-41e6-957f-cb198a7821d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Telecom Domain Read & Write Ops - Building Datalake & Lakehouse\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes. <br>\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f3d426-64b6-48ed-9037-daff49616bb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.session import SparkSession\n",
    "print(spark)#already instantiated by databricks\n",
    "spark1=SparkSession.builder.getOrCreate()\n",
    "print(spark1)#we instantiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429f2f81-1faf-442e-b60e-2e6d011bba4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists telecom_catalog_assign;\n",
    "create schema if not exists telecom_catalog_assign.landing_zone;\n",
    "create volume if not exists telecom_catalog_assign.landing_zone.landing_vol;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60380b35-e3f5-489c-9376-0ed23509b5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for folder in [\n",
    "      \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\",\n",
    "]:\n",
    "  dbutils.fs.mkdirs(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b347431-c698-4f27-adf2-68bf3687cafa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for subfolder in [\n",
    "       \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2\"\n",
    "]:\n",
    "  dbutils.fs.mkdirs(subfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf59a70d-8915-4182-92e9-2fb711543792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####DBFS\n",
    "1. DBFS / FileStore (Old approach)\n",
    "**What it is**\n",
    "DBFS (Databricks File System) is a workspace-level file abstraction\n",
    "FileStore is a publicly accessible subfolder of DBFS\n",
    "Mainly designed for experimentation, demos, notebooks\n",
    "**Key characteristics**\n",
    "Not governed by Unity Catalog\n",
    "No fine-grained access control (only workspace-level permissions)\n",
    "No table-level or column-level lineage\n",
    "FileStore files can be exposed via public URLs\n",
    "Weak auditability\n",
    "**Typical usage**\n",
    "Temporary files\n",
    "Sample datasets\n",
    "Notebook outputs\n",
    "Quick testing\n",
    "**Why it‚Äôs not prod-ready**\n",
    "‚ùå No centralized governance\n",
    "‚ùå No row/column/file-level security\n",
    "‚ùå Hard to audit ‚Äúwho accessed what‚Äù\n",
    "‚ùå Not compliant for sensitive data\n",
    "\n",
    "#### VOLUMES\n",
    "**What it is**\n",
    "Volumes are governed storage objects under Unity Catalog\n",
    "They provide secure file storage similar to tables\n",
    "Backed by cloud storage (ADLS / S3 / GCS)\n",
    "\n",
    "**Key characteristics**\n",
    "Fully integrated with Unity Catalog\n",
    "Supports fine-grained access control\n",
    "Audited and tracked\n",
    "Secure, no public URLs\n",
    "Clear ownership and lifecycle management\n",
    "**Typical usage**\n",
    "Ingestion landing zones\n",
    "Raw / bronze data\n",
    "ML artifacts\n",
    "Regulated datasets\n",
    "Production pipelines\n",
    "**Why it is prod-ready**\n",
    "‚úÖ Central governance\n",
    "‚úÖ Strong security & compliance\n",
    "‚úÖ Auditing & lineage\n",
    "‚úÖ Works across all Databricks workspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ea00286-8575-4a11-b739-67749b6eefb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### why volume instead of dbfs?\n",
    "DBFS/FileStore is meant for development and experimentation, while Volumes are Unity Catalog‚Äìgoverned, secure, auditable storage objects designed for production and regulated data. Production teams prefer Volumes because they provide fine-grained access control, auditability, and compliance that DBFS cannot offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11bcaaa1-fb36-485d-810f-1f3e62ebcc78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### b. Why production teams prefer Volumes for regulated data?\n",
    "**1. Regulated data needs governance**\n",
    "Regulated data includes:\n",
    "PII (Aadhaar, PAN, phone, email)\n",
    "Financial data\n",
    "Healthcare data\n",
    "Customer records\n",
    "Production teams must answer:\n",
    "Who accessed this data?\n",
    "When was it accessed?\n",
    "Was access authorized?\n",
    "‚û°Ô∏è DBFS cannot answer these questions reliably\n",
    "‚û°Ô∏è Volumes can\n",
    "\n",
    "**2. Fine-grained access control (critical)**\n",
    "With Volumes, teams can:\n",
    "GRANT READ FILES ON VOLUME main.sales.raw_data TO analyst_role;\n",
    "This means:\n",
    "Only authorized roles can read/write\n",
    "Access can be revoked instantly\n",
    "No accidental exposure\n",
    "\n",
    "DBFS:\n",
    "Either you have workspace access or you don‚Äôt\n",
    "No file-level control\n",
    "\n",
    "**3. Audit & compliance (non-negotiable)**\n",
    "Regulators require:\n",
    "Audit logs\n",
    "Access history\n",
    "Ownership tracking\n",
    "Volumes provide:\n",
    "‚úÖ Who accessed which file\n",
    "‚úÖ Which pipeline wrote the data\n",
    "‚úÖ When access happened\n",
    "DBFS:\n",
    "‚ùå Weak or no audit trail\n",
    "\n",
    "**4. Separation of concerns (clean architecture)**\n",
    "Raw data  ‚Üí  Processed data  ‚Üí  Curated data\n",
    "Volumes help enforce this:\n",
    "Raw volumes (restricted)\n",
    "Processed volumes (controlled)\n",
    "Curated tables (consumer-facing)\n",
    "DBFS mixes everything ‚Üí chaos in prod.\n",
    "\n",
    "**5. Future-proof & multi-workspace support**\n",
    "\n",
    "Volumes:\n",
    "Work across multiple Databricks workspaces\n",
    "Central governance via Unity Catalog\n",
    "Scales for enterprise growth\n",
    "\n",
    "DBFS:\n",
    "Tied to a single workspace\n",
    "Legacy approach\n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca04d99a-4059-478f-b7cc-f38fb60bb3b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "# we use put as we are dealing with smaller dataset\n",
    "#dbutils.fs.put() is used to create or overwrite small text-based files in Databricks storage (DBFS or Volumes) by writing string content directly.\n",
    "'''Because overwrite=False is a safety flag:\n",
    "Prevents accidental data loss\n",
    "Forces you to explicitly allow replacement\n",
    "When overwrite=False, dbutils.fs.put() will fail if the file already exists, protecting against accidental overwrites; when True, it replaces the existing file.\n",
    "'''\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",customer_csv,overwrite=True)\n",
    "\n",
    "#tsv-tab separeated values\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",usage_tsv,overwrite=True)\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\",tower_logs_region1,overwrite=True)\n",
    "\n",
    "tower_logs_region2 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "6001|102|TWR01|-90|2025-02-15 10:21:54\n",
    "6004|106|TWR05|-55|2025-02-15 11:01:12\n",
    "'''\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/tower_logs_region2.csv\",tower_logs_region1,overwrite=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29e5d8ee-2af5-491c-b559-02a0ab04fe78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower\"\n",
    "]\n",
    "\n",
    "for path in paths:\n",
    "    files = dbutils.fs.ls(path)\n",
    "    if files:\n",
    "        print(f\"{path} ‚Üí {len(files)} files found\")\n",
    "    else:\n",
    "        print(f\"{path} ‚Üí No files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcef5ce6-99dc-41a0-8601-78e0b1767a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read all tower logs using: Path glob filter (example: *.csv) Multiple paths input Recursive lookup\n",
    "'''\n",
    "Purpose: Read all files in a folder and its nested subfolders.\n",
    "Default behavior: Spark only reads files in the top-level folder.\n",
    "Option: Set .option(\"recursiveFileLookup\", \"true\") to include subfolders.\n",
    "'''\n",
    "df=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\",recursiveFileLookup=True)\n",
    "print(f\"Total rows in all tower logs: {df.count()}\")\n",
    "#Demonstrate these 3 reads separately: Using pathGlobFilter Using list of paths in spark.read.csv([path1, path2]) Using .option(\"recursiveFileLookup\",\"true\")\n",
    "df=spark.read.option(\"recursiveFileLookup\",\"true\").csv(['/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv','/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/tower_logs_region2.csv'])\n",
    "print(f\"Total rows in all tower logs: {df.count()}\")\n",
    "\n",
    "df=spark.read.options(header=True,inferSchema=True,recursiveFileLookup=True,pathGlobFilter=\"*.csv\",sep='|').format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b55ce4-407a-4b24-ae71-f942103171e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''Try the Customer, Usage files with the option and options using read.csv and format function:\n",
    "header=false, inferSchema=false\n",
    "or\n",
    "header=true, inferSchema=true'''\n",
    "df=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",header=True,inferSchema=False)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "df=spark.read.options(header=True,inferSchema=True).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "#if infereschema is false all the columns will be treated as string,and also the first row of the dataset is set as the header check the columns names\n",
    "#How schema inference handled ‚Äúabc‚Äù in age? it is treated as a string column instead of int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460e9bdb-f35a-4262-b1d2-5311a92459a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Apply column names using string using toDF function for customer data\n",
    "df=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",header=False,inferSchema=True).toDF(\"customer_id\",\"name\",\"age\",\"city\",\"plan\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "#Apply column names and datatype using the schema function for usage data\n",
    "schema_data=\"\"\"\n",
    "customer_id int,\n",
    "name string,\n",
    "age string,\n",
    "city string\n",
    "\"\"\"\n",
    "df=spark.read.schema(schema_data).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",header=True,sep=\"\\t\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "    # IF HEADER FALSE IN HERE FIRST ROW IS HEADER INFO GIVEN IN DATASET BECOZ OF WHICH IT ADDED AS ROW IN THE DATAFRAME\n",
    "    #||||here is nullable is not given so we can zero added in those places||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e327c1-e73c-47ae-8356-07d07d06b5e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data\n",
    "from pyspark.sql.types import *\n",
    "schema_data=StructType([\n",
    "    StructField(\"event_id\",IntegerType(),True),\n",
    "    StructField(\"customer_id\",IntegerType(),True),\n",
    "    StructField(\"tower_id\",StringType(),True),\n",
    "    StructField(\"signalstrength\",IntegerType(),True),\n",
    "    StructField(\"timestamp\",TimestampType(),True)]\n",
    ")\n",
    "\n",
    "df1=spark.read.schema(schema_data).csv(f\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\",header=True,sep=\"|\")\n",
    "df1.printSchema()\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8145630-7d59-499f-a869-d04777e6e338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####write operations\n",
    "##### Spark Write Operations using \n",
    "- csv, json, orc, parquet, delta, saveAsTable, insertInto, xml with different write mode, header and sep options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bb5902a-d770-40c8-b196-dbd537fbb80f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS telecom_catalog_assign.landing_zone.ingestion_volume;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "438897ca-072b-42b6-bfba-0da95094bb3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create volume if not exists telecom_catalog_assign.landing_zone.ingestion_volume;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f66a32db-6917-449b-9a38-b18485433b2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inc_df=spark.read.csv('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv',header=True,inferSchema=True)\n",
    "inu_df=spark.read.csv('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv',header=True,inferSchema=True)\n",
    "int_df=spark.read.csv('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/tower_logs_region2.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb372f3-74ff-49ce-8e69-460eda3016eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#csv write operations in different modes\n",
    "\n",
    "\n",
    "inc_df.write.csv(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/cus_csv_out\",header=True,sep=\"|\",mode=\"overwrite\")\n",
    "inu_df.write.csv(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/usage_csv_out\",header=True,sep=\"|\",mode=\"append\")\n",
    "int_df.write.csv(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/tower_csv_out\",header=True,sep=\"|\",mode=\"overwrite\")\n",
    "#4 modes of writing - append,overwrite,ignore,error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51e317b5-6bfd-4d39-9c23-26f53991476e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#orc operations\n",
    "inc_df.write.orc(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/cus_orc_out\",mode=\"overwrite\",compression=\"zlib\")\n",
    "inu_df.write.mode(\"append\").orc(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/usageorc_out\")\n",
    "#This is a Unity Catalog Volume (not a regular path on DBFS).\n",
    "#Volumes are special storage mounts, they do not support all Spark write modes, including errorIfExists.\n",
    "#Even though Parquet normally supports errorIfExists, Volumes only allow ‚Äúappend‚Äù or ‚Äúoverwrite‚Äù writes, not errorIfExists or Delta-specific modes.\n",
    "int_df.write.orc(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/tower_orc_out\",mode=\"overwrite\",compression=\"zlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38158a4d-0fd5-4da6-b27f-8d2d1c06ed33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#parquet operations\n",
    "inc_df.write.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/cus_p_out\",mode=\"overwrite\",compression=\"gzip\")\n",
    "inu_df.write.mode(\"append\").parquet(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/usagep_out\")\n",
    "#This is a Unity Catalog Volume (not a regular path on DBFS).\n",
    "#Volumes are special storage mounts, they do not support all Spark write modes, including errorIfExists.\n",
    "#Even though Parquet normally supports errorIfExists, Volumes only allow ‚Äúappend‚Äù or ‚Äúoverwrite‚Äù writes, not errorIfExists or Delta-specific modes.\n",
    "int_df.write.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/tower_p_out\",mode=\"overwrite\",compression=\"gzip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559f6706-5d8c-4d83-97a7-d0e6b54e5386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#json operations\n",
    "inc_df.write.option(\"pretty\", \"true\").json(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/cus_json_out\",mode=\"overwrite\")\n",
    "inu_df.write.option(\"pretty\", \"true\").json(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/usage_json_out\",mode=\"append\",compression=\"snappy\")\n",
    "int_df.write.option(\"pretty\", \"true\").json(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/tower_json_out\",mode=\"ignore\")\n",
    "int_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950cbfa1-3e51-4e65-aadb-dcc81259919c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#lakehouse operations\n",
    "inc_df.write.format(\"delta\").saveAsTable(\"telecom_catalog_assign.landing_zone.ingest_c_table\",mode=\"overwrite\")\n",
    "#inu_df.write.format(\"delta\").saveAsTable(\"telecom_catalog_assign.landing_zone.ingest_u_table\",mode=\"append\")\n",
    "int_df.write.format(\"delta\").saveAsTable(\"telecom_catalog_assign.landing_zone.ingest_t_table\",mode=\"append\")\n",
    "'''Delta Lake does not allow these modes:\n",
    "errorIfExists\n",
    "ignore\n",
    "Writing Delta directly to Volume (NOT allowed)\n",
    "Unity Catalog Volumes are storage-only and do not support Delta transaction logs, so Delta tables must be written as managed or external tables‚Äînot directly to Volumes.'''\n",
    "#load() ‚Üí READ data (Input)\n",
    "#save() ‚Üí WRITE data (Output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "332ce33e-c897-4dd5-86f1-2e2eb7b3dd43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####LAKEHOUSE OPERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c03f0434-a36e-469c-aacd-648b5c5efeb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "inc_df_fixed = inc_df.select(\n",
    "    col(\"id\").cast(\"STRING\"),\n",
    "    col(\"name\").cast(\"STRING\"),\n",
    "    expr(\"try_cast(age as INT)\").alias(\"age\"),\n",
    "    col(\"city\").cast(\"STRING\"),\n",
    "    col(\"plan\").cast(\"STRING\")\n",
    ")\n",
    "\n",
    "inc_df_fixed.write.mode(\"overwrite\").insertInto(\n",
    "    \"telecom_catalog_assign.landing_zone.bronze_tablenew\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f46a3541-29e3-4e30-90f5-5e398506671e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove invalid characters (e.g., tabs, spaces) from column names\n",
    "inu_df_clean = inu_df.toDF(\n",
    "    *[c.replace('\\t', '_').replace(' ', '_') for c in inu_df.columns]\n",
    ")\n",
    "\n",
    "inu_df_clean.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"telecom_catalog_assign.landing_zone.usage_table\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6967bdfa-3687-4bcb-88d4-691fd63ecb37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inc_df.write.xml(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/cust_xml_out\",mode=\"overwrite\",rowTag=\"customer\")\n",
    "inc_df.write.xml(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/usage_xml_out\",mode=\"overwrite\",rowTag=\"usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16dffa78-8687-44ed-aeea-63138906cf89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### csv-167 b\n",
    "#### json-494 b\n",
    "#### orc-836 b\n",
    "#### parquet-1.84b\n",
    "#### xml-837 b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "739c1935-051b-41a4-bd71-9e076095a968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìä Data File Formats in Spark ‚Äì When to Use & Benefits\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ CSV (Comma Separated Values)\n",
    "\n",
    "### ‚úÖ When to Use\n",
    "- Simple data exchange between systems\n",
    "- Small datasets\n",
    "- When human readability is required\n",
    "- Legacy system compatibility\n",
    "\n",
    "### ‚≠ê Benefits\n",
    "- Easy to read and write\n",
    "- Supported by almost all tools\n",
    "- Lightweight and simple\n",
    "\n",
    "### ‚ùå Limitations\n",
    "- No schema enforcement\n",
    "- No compression by default\n",
    "- Poor performance on large datasets\n",
    "- Does not support nested structures\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ JSON\n",
    "\n",
    "### ‚úÖ When to Use\n",
    "- Semi-structured data\n",
    "- API responses, logs, streaming data\n",
    "- Frequently changing schema\n",
    "\n",
    "### ‚≠ê Benefits\n",
    "- Supports nested and hierarchical data\n",
    "- Flexible schema\n",
    "- Human readable\n",
    "\n",
    "### ‚ùå Limitations\n",
    "- Large file size\n",
    "- Slower parsing\n",
    "- Not efficient for analytics\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ ORC (Optimized Row Columnar)\n",
    "\n",
    "### ‚úÖ When to Use\n",
    "- Hive-based data warehouses\n",
    "- Large analytical workloads\n",
    "- Hadoop ecosystem\n",
    "\n",
    "### ‚≠ê Benefits\n",
    "- Columnar storage\n",
    "- High compression\n",
    "- Predicate pushdown\n",
    "- Faster query performance\n",
    "\n",
    "### ‚ùå Limitations\n",
    "- Limited support outside Hive ecosystem\n",
    "- Not suitable for frequent updates\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Parquet\n",
    "\n",
    "### ‚úÖ When to Use\n",
    "- Analytics and reporting workloads\n",
    "- Spark, Databricks, cloud platforms\n",
    "- Read-heavy processing\n",
    "\n",
    "### ‚≠ê Benefits\n",
    "- Columnar format\n",
    "- Efficient compression\n",
    "- Faster reads\n",
    "- Schema stored with data\n",
    "\n",
    "### ‚ùå Limitations\n",
    "- No ACID transactions\n",
    "- Poor support for updates/deletes\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Delta (Delta Lake Format)\n",
    "\n",
    "### ‚úÖ When to Use\n",
    "- Reliable data lakes\n",
    "- Data with frequent changes\n",
    "- Production pipelines\n",
    "\n",
    "### ‚≠ê Benefits\n",
    "- Built on Parquet\n",
    "- ACID transactions\n",
    "- Schema enforcement and evolution\n",
    "- Time travel support\n",
    "\n",
    "### ‚ùå Limitations\n",
    "- Slight write overhead\n",
    "- Requires Delta-compatible engines\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ XML\n",
    "\n",
    "### ‚úÖ When to Use\n",
    "- Legacy enterprise systems\n",
    "- Configuration files\n",
    "- Data exchange standards (SOAP)\n",
    "\n",
    "### ‚≠ê Benefits\n",
    "- Strong schema validation (XSD)\n",
    "- Self-describing structure\n",
    "\n",
    "### ‚ùå Limitations\n",
    "- Very verbose\n",
    "- Large file size\n",
    "- Slow processing\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Delta Tables (Managed Delta Lake Tables)\n",
    "\n",
    "### ‚úÖ When to Use\n",
    "- Enterprise-grade data platforms\n",
    "- Unity Catalog enabled environments\n",
    "- Multi-user analytics\n",
    "\n",
    "### ‚≠ê Benefits\n",
    "- All Delta Lake advantages\n",
    "- Table-level security\n",
    "- Data lineage and governance\n",
    "- SQL and Spark interoperability\n",
    "\n",
    "### ‚ùå Limitations\n",
    "- Platform dependent (Databricks preferred)\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary Comparison\n",
    "\n",
    "| Format | Storage | ACID | Compression | Schema | Updates |\n",
    "|------|--------|------|------------|--------|---------|\n",
    "| CSV | Row | ‚ùå | ‚ùå | ‚ùå | ‚ùå |\n",
    "| JSON | Row | ‚ùå | ‚ùå | Flexible | ‚ùå |\n",
    "| XML | Row | ‚ùå | ‚ùå | Strong | ‚ùå |\n",
    "| ORC | Column | ‚ùå | ‚úÖ | ‚úÖ | ‚ùå |\n",
    "| Parquet | Column | ‚ùå | ‚úÖ | ‚úÖ | ‚ùå |\n",
    "| Delta | Column | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |\n",
    "| Delta Table | Column | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Quick Tip\n",
    "- **CSV / JSON / XML** ‚Üí Data ingestion & exchange  \n",
    "- **Parquet / ORC** ‚Üí Analytics & performance  \n",
    "- **Delta / Delta Tables** ‚Üí Production-ready lakehouse  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63b1448d-e2e2-4396-b2c9-f34ad6100262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####reading and writing the files in different combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed990cb1-5265-4e89-b37b-0f0c5569c959",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "e"
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"orc\").load(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/cus_orc_out\").write.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/cus_orc_parquet_out\",mode=\"overwrite\")\n",
    "spark.read.format(\"parquet\").load(\"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/cus_p_out\").write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"telecom_catalog_assign.landing_zone.p_to_d_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fd69497-98b4-4fe4-b556-c16cf500d8b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"telecom_catalog_assign.landing_zone.p_to_d_table\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\n",
    "    \"telecom_catalog_assign.landing_zone.d_to_d_table\"\n",
    ")\n",
    "# use spark.read to read a delta table which is already added under table"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6048219941612287,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "read and write operations 2 updated",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
