{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "259d0239-7f2a-4009-8680-0b860d6336de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####CAPSTONE PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b0ef291-14b4-4a02-9fac-29fbb5e4fed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c3828cb-8df3-43bd-ab5c-5cc7a62d03a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd59ceab-4aa7-4329-85ff-e1f82fa24f39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create schema if not exists lakehouse.lakehouse_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2db2045-5fae-4de8-8dc7-0f9fe6bd1dce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create volume if not exists lakehouse.lakehouse_schema.lakehouse_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "882766de-9b0e-4371-905e-1e18e757d289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####REFERENCE:\n",
    "take()-Purpose: Fetch data into the driver program<br>\n",
    "show()-Purpose: Display data in a readable table format (mainly for debugging/inspection)<br>\n",
    "display()-Purpose: Rich, interactive visualization of data (Databricks-only)<br>\n",
    "sample()-\n",
    "\n",
    "syntam: df.sample(withReplacement=False, fraction=0.1, seed=42)\n",
    "true-->can repeat ,false-->will not repeat\n",
    "seed-->make same randomness for others if they use the same seed number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aefc719f-90de-4ee0-ae51-b4ed24df1f00",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767024325000}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.csv(\"/Volumes/lakehouse/lakehouse_schema/lakehouse_volume/custsmodified\",header=False,inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"profession\")\n",
    "#df.show(20,True)\n",
    "display(df.take(20))\n",
    "display(df.sample(.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e3aea1-853e-4970-820b-baa415c73768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "print(df.schema) #- to identify the data using structtype and structfield format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f1ee69f-b070-4689-b1dc-6711e1f2256f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - \n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "print(df.distinct().count())#row level \n",
    "display(df.distinct())#row level if id is same and name is different in 2 row means it will be treated as distinct only\n",
    "print(df.dropDuplicates([\"id\"]).count()) #column level\n",
    "display(df.dropDuplicates([\"id\"]))\n",
    "print(df.describe())\n",
    "print(df.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d57e44f2-f5d0-4bb8-a1e6-7d912f008951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#structuring of data -combining data + schema\n",
    "struct1=\"id string,firstname string,lastname string,age string,profession string\"\n",
    "rawdf=spark.read.schema(struct1).csv(\"/Volumes/lakehouse/lakehouse_schema/lakehouse_volume/custsmodified\",header=False,inferSchema=True)\n",
    "print(rawdf.schema)\n",
    "#multiple lines with same or different names\n",
    "df=spark.read.schema(struct1).csv(path=[\"/Volumes/lakehouse/lakehouse_schema/lakehouse_volume/custsmodified\",\"/Volumes/lakehouse/lakehouse_schema/lakehouse_volume/custsmodified\"])\n",
    "#multiple files in mulitple path\n",
    "df=spark.read.schema(struct1).csv(path=[\"/Volumes/lakehouse/lakehouse_schema/lakehouse_volume/custsmodified\",\"/Volumes/telecom/telecom_schema/telecom_volume/custsmodified\"],recursiveFileLookup=True,pathGlobalFilter=\"cust_*\")\n",
    "\n",
    "#pattern of files is mentioned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9e7fbfc-8ff5-4115-808e-8e49e4cd7dfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### structuring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03a61e94-1d49-4fd4-b063-6c2207a92693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "struct1 = \"id string,firstname string,lastname string,age string,profession string\"\n",
    "\n",
    "rawdf1 = spark.read.schema(struct1).csv(\n",
    "    \"/Volumes/lakehouse/lakehouse_schema/lakehouse_volume/custsmodified\",\n",
    "    header=False\n",
    ")\n",
    "rawdf2 = spark.read.schema(struct1).csv(\n",
    "    \"/Volumes/lakehouse/lakehouse_schema/lakehouse_volume/custsmodified\",\n",
    "    header=False,\n",
    "    mode=\"dropMalformed\"\n",
    ")\n",
    "\n",
    "rawdf3 = rawdf1.union(rawdf2)\n",
    "#)#Use union only if the dataframes are having same columns in the same order with same datatype\n",
    "struct3 = \"id string,name string,age string,city string,plan string\"\n",
    "rawdf4 = spark.read.schema(struct3).orc(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/ingestion_volume/cus_orc_out/\"\n",
    ").toDF(\"id\", \"name\", \"age\", \"city\", \"plan\")\n",
    "\n",
    "rawdf5 = rawdf2.unionByName(\n",
    "    rawdf4,\n",
    "    allowMissingColumns=True\n",
    ")\n",
    "\n",
    "display(rawdf5)\n",
    "#files should have same data columns or even new columns can be added but here these are 2 different fileswith nolink so showing null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f327bf-b0bc-486c-b33a-6f0c84246ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e678bc5-aebb-4d99-8bb2-11551c055b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,ShortType\n",
    "stype=StructType([\n",
    "    StructField(\"id\",IntegerType(),True),\n",
    "    StructField(\"firstname\",StringType(),True),\n",
    "    StructField(\"lastname\",StringType(),True),\n",
    "    StructField(\"age\",ShortType(),True),\n",
    "    StructField(\"profession\",StringType(),True)\n",
    "])\n",
    "cleandf=spark.read.schema(stype).csv(\"/Volumes/lakehouse/lakehouse_schema/lakehouse_volume/custsmodified\",mode=\"permissive\")\n",
    "print(\"after keeping nulls on the wrong data format\",cleandf.count())#all rows count\n",
    "display(cleandf)#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato) or\n",
    "cleandf=spark.read.schema(stype).csv(\"/Volumes/lakehouse/lakehouse_schema/lakehouse_volume/custsmodified\",mode=\"dropMalformed\")\n",
    "print(\"after cleaning wrong data (type mismatch, column number mismatch)\",len(cleandf.collect()))\n",
    "display(cleandf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac4061b8-62f0-4f9b-be20-7a94ea6ef45f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#collect() is an action in PySpark that:\n",
    "#Triggers execution of all previous transformations\n",
    "#Brings the entire DataFrame / RDD data from executors to the driver\n",
    "#Returns the data as a Python list ,make sure the data is 100% small\n",
    "“When collect() is called, Spark executes one task per partition on executors, processes the data there, serializes each partition’s result, sends it over the network, and finally materializes everything in the driver’s memory.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e53425eb-addf-4588-9c2e-88231f2ce3d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ShortType\n",
    "\n",
    "stype = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"age\", ShortType(), True),\n",
    "    StructField(\"profession\", StringType(), True)\n",
    "])\n",
    "\n",
    "cleandf = spark.read.schema(stype).csv(\n",
    "    \"/Volumes/lakehouse/lakehouse_schema/lakehouse_volume/custsmodified\",\n",
    "    mode=\"permissive\",\n",
    "    columnNameOfCorruptRecord=\"corrupt_record\"\n",
    ")\n",
    "\n",
    "print(\"after keeping nulls on the wrong data format\", cleandf.count())\n",
    "cleandf.printSchema()\n",
    "display(cleandf)\n",
    "#rejecteddf1 = cleandf.where(\"corrupt_record is not null\")-->this filters only the rows with corrupt_record\n",
    "#print(\"corrupted record dataset\", rejecteddf1.count())\n",
    "#display(rejecteddf1) error while running this becoz of no corruptes records so no column is created \n",
    "#retaineddf = cleandf.where(\"corrupt_record is null\")-->this filters only the rows with corrupt_record\n",
    "#print(\"corrupted record dataset\", retaineddf.count())\n",
    "#print(\"Overall rows in the source data is \",len(cleandf.collect()))\n",
    "#print(\"Rejected rows in the source data is \",len(rejectdf.collect()))\n",
    "#print(\"Clean rows in the source data is \",len(retaineddf1.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21448b68-7fbb-4194-a78e-bcf31548fccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#cleaning or cleansing -removing or deleting the corrupted records\n",
    "cleandf2=cleandf.na.drop(how='any')# drops the row if any one of the column value is null\n",
    "cleandf2=cleandf.na.drop(how='all')#drops the row only if all the column values are null\n",
    "cleandf2=cleandf.na.drop(how='any',subset=['id','firstname'])#drops the row if any one of the column value is null in the subset columns\n",
    "cleandf2=cleandf.na.drop(how='all',subset=['id','firstname'])#drops the row only if all the column values are null in the subset columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09f4d510-ec51-496d-8f81-2929b00b2897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#scrubbing- making the existing data tidy\n",
    "cleandfnew=cleandf2.na.fill(\"not provided\",subset=['profession','age'])\n",
    "display(cleandfnew)\n",
    "f_r={\"Pilot\":\"Captain\"}\n",
    "cleandfnew=cleandf2.na.replace(f_r,subset=['profession'])\n",
    "display(cleandfnew)\n",
    "#as the name replace explains it searches the word and then replaces it as per provided in the disctionary"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6844536264606713,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "capstone project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
