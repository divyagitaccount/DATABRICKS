{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eef4cbb-25c7-41e6-957f-cb198a7821d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### READ AND WRITE OPERATIONS PART 2\n",
    "####---------- TELECOM OPERATIONS --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f3d426-64b6-48ed-9037-daff49616bb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.session import SparkSession\n",
    "print(spark)#already instantiated by databricks\n",
    "spark1=SparkSession.builder.getOrCreate()\n",
    "print(spark1)#we instantiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429f2f81-1faf-442e-b60e-2e6d011bba4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists telecom_catalog_assign;\n",
    "create schema if not exists telecom_catalog_assign.landing_zone;\n",
    "create volume if not exists telecom_catalog_assign.landing_zone.landing_vol;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60380b35-e3f5-489c-9376-0ed23509b5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for folder in [\n",
    "      \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\",\n",
    "]:\n",
    "  dbutils.fs.mkdirs(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b347431-c698-4f27-adf2-68bf3687cafa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for subfolder in [\n",
    "       \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2\"\n",
    "]:\n",
    "  dbutils.fs.mkdirs(subfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf59a70d-8915-4182-92e9-2fb711543792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####DBFS\n",
    "1. DBFS / FileStore (Old approach)\n",
    "**What it is**\n",
    "DBFS (Databricks File System) is a workspace-level file abstraction\n",
    "FileStore is a publicly accessible subfolder of DBFS\n",
    "Mainly designed for experimentation, demos, notebooks\n",
    "**Key characteristics**\n",
    "Not governed by Unity Catalog\n",
    "No fine-grained access control (only workspace-level permissions)\n",
    "No table-level or column-level lineage\n",
    "FileStore files can be exposed via public URLs\n",
    "Weak auditability\n",
    "**Typical usage**\n",
    "Temporary files\n",
    "Sample datasets\n",
    "Notebook outputs\n",
    "Quick testing\n",
    "**Why it’s not prod-ready**\n",
    "❌ No centralized governance\n",
    "❌ No row/column/file-level security\n",
    "❌ Hard to audit “who accessed what”\n",
    "❌ Not compliant for sensitive data\n",
    "\n",
    "#### VOLUMES\n",
    "**What it is**\n",
    "Volumes are governed storage objects under Unity Catalog\n",
    "They provide secure file storage similar to tables\n",
    "Backed by cloud storage (ADLS / S3 / GCS)\n",
    "\n",
    "**Key characteristics**\n",
    "Fully integrated with Unity Catalog\n",
    "Supports fine-grained access control\n",
    "Audited and tracked\n",
    "Secure, no public URLs\n",
    "Clear ownership and lifecycle management\n",
    "**Typical usage**\n",
    "Ingestion landing zones\n",
    "Raw / bronze data\n",
    "ML artifacts\n",
    "Regulated datasets\n",
    "Production pipelines\n",
    "**Why it is prod-ready**\n",
    "✅ Central governance\n",
    "✅ Strong security & compliance\n",
    "✅ Auditing & lineage\n",
    "✅ Works across all Databricks workspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ea00286-8575-4a11-b739-67749b6eefb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### why volume instead of dbfs?\n",
    "DBFS/FileStore is meant for development and experimentation, while Volumes are Unity Catalog–governed, secure, auditable storage objects designed for production and regulated data. Production teams prefer Volumes because they provide fine-grained access control, auditability, and compliance that DBFS cannot offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11bcaaa1-fb36-485d-810f-1f3e62ebcc78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### b. Why production teams prefer Volumes for regulated data?\n",
    "**1. Regulated data needs governance**\n",
    "Regulated data includes:\n",
    "PII (Aadhaar, PAN, phone, email)\n",
    "Financial data\n",
    "Healthcare data\n",
    "Customer records\n",
    "Production teams must answer:\n",
    "Who accessed this data?\n",
    "When was it accessed?\n",
    "Was access authorized?\n",
    "➡️ DBFS cannot answer these questions reliably\n",
    "➡️ Volumes can\n",
    "\n",
    "**2. Fine-grained access control (critical)**\n",
    "With Volumes, teams can:\n",
    "GRANT READ FILES ON VOLUME main.sales.raw_data TO analyst_role;\n",
    "This means:\n",
    "Only authorized roles can read/write\n",
    "Access can be revoked instantly\n",
    "No accidental exposure\n",
    "\n",
    "DBFS:\n",
    "Either you have workspace access or you don’t\n",
    "No file-level control\n",
    "\n",
    "**3. Audit & compliance (non-negotiable)**\n",
    "Regulators require:\n",
    "Audit logs\n",
    "Access history\n",
    "Ownership tracking\n",
    "Volumes provide:\n",
    "✅ Who accessed which file\n",
    "✅ Which pipeline wrote the data\n",
    "✅ When access happened\n",
    "DBFS:\n",
    "❌ Weak or no audit trail\n",
    "\n",
    "**4. Separation of concerns (clean architecture)**\n",
    "Raw data  →  Processed data  →  Curated data\n",
    "Volumes help enforce this:\n",
    "Raw volumes (restricted)\n",
    "Processed volumes (controlled)\n",
    "Curated tables (consumer-facing)\n",
    "DBFS mixes everything → chaos in prod.\n",
    "\n",
    "**5. Future-proof & multi-workspace support**\n",
    "\n",
    "Volumes:\n",
    "Work across multiple Databricks workspaces\n",
    "Central governance via Unity Catalog\n",
    "Scales for enterprise growth\n",
    "\n",
    "DBFS:\n",
    "Tied to a single workspace\n",
    "Legacy approach\n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca04d99a-4059-478f-b7cc-f38fb60bb3b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "# we use put as we are dealing with smaller dataset\n",
    "#dbutils.fs.put() is used to create or overwrite small text-based files in Databricks storage (DBFS or Volumes) by writing string content directly.\n",
    "'''Because overwrite=False is a safety flag:\n",
    "Prevents accidental data loss\n",
    "Forces you to explicitly allow replacement\n",
    "When overwrite=False, dbutils.fs.put() will fail if the file already exists, protecting against accidental overwrites; when True, it replaces the existing file.\n",
    "'''\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",customer_csv,overwrite=True)\n",
    "\n",
    "#tsv-tab separeated values\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",usage_tsv,overwrite=True)\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\",tower_logs_region1,overwrite=True)\n",
    "\n",
    "tower_logs_region2 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "6001|102|TWR01|-90|2025-02-15 10:21:54\n",
    "6004|106|TWR05|-55|2025-02-15 11:01:12\n",
    "'''\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/tower_logs_region2.csv\",tower_logs_region1,overwrite=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29e5d8ee-2af5-491c-b559-02a0ab04fe78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower\"\n",
    "]\n",
    "\n",
    "for path in paths:\n",
    "    files = dbutils.fs.ls(path)\n",
    "    if files:\n",
    "        print(f\"{path} → {len(files)} files found\")\n",
    "    else:\n",
    "        print(f\"{path} → No files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcef5ce6-99dc-41a0-8601-78e0b1767a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read all tower logs using: Path glob filter (example: *.csv) Multiple paths input Recursive lookup\n",
    "'''\n",
    "Purpose: Read all files in a folder and its nested subfolders.\n",
    "Default behavior: Spark only reads files in the top-level folder.\n",
    "Option: Set .option(\"recursiveFileLookup\", \"true\") to include subfolders.\n",
    "'''\n",
    "df=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\",recursiveFileLookup=True)\n",
    "print(f\"Total rows in all tower logs: {df.count()}\")\n",
    "#Demonstrate these 3 reads separately: Using pathGlobFilter Using list of paths in spark.read.csv([path1, path2]) Using .option(\"recursiveFileLookup\",\"true\")\n",
    "df=spark.read.option(\"recursiveFileLookup\",\"true\").csv(['/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv','/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/tower_logs_region2.csv'])\n",
    "print(f\"Total rows in all tower logs: {df.count()}\")\n",
    "\n",
    "df=spark.read.options(header=True,inferSchema=True,recursiveFileLookup=True,pathGlobFilter=\"*.csv\",sep='|').format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b55ce4-407a-4b24-ae71-f942103171e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''Try the Customer, Usage files with the option and options using read.csv and format function:\n",
    "header=false, inferSchema=false\n",
    "or\n",
    "header=true, inferSchema=true'''\n",
    "df=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",header=True,inferSchema=False)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "df=spark.read.options(header=True,inferSchema=True).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "#if infereschema is false all the columns will be treated as string,and also the first row of the dataset is set as the header check the columns names\n",
    "#How schema inference handled “abc” in age? it is treated as a string column instead of int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460e9bdb-f35a-4262-b1d2-5311a92459a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Apply column names using string using toDF function for customer data\n",
    "df=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",header=False,inferSchema=True).toDF(\"customer_id\",\"name\",\"age\",\"city\",\"plan\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "#Apply column names and datatype using the schema function for usage data\n",
    "schema_data=\"\"\"\n",
    "customer_id int,\n",
    "name string,\n",
    "age string,\n",
    "city string\n",
    "\"\"\"\n",
    "df=spark.read.schema(schema_data).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",header=True,sep=\"\\t\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "    # IF HEADER FALSE IN HERE FIRST ROW IS HEADER INFO GIVEN IN DATASET BECOZ OF WHICH IT ADDED AS ROW IN THE DATAFRAME\n",
    "    #||||here is nullable is not given so we can zero added in those places||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e327c1-e73c-47ae-8356-07d07d06b5e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data\n",
    "from pyspark.sql.types import *\n",
    "schema_data=StructType([\n",
    "    StructField(\"event_id\",IntegerType(),True),\n",
    "    StructField(\"customer_id\",IntegerType(),True),\n",
    "    StructField(\"tower_id\",StringType(),True),\n",
    "    StructField(\"signalstrength\",IntegerType(),True),\n",
    "    StructField(\"timestamp\",TimestampType(),True)]\n",
    ")\n",
    "\n",
    "df=spark.read.schema(schema_data).csv(f\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\",header=True,sep=\"|\")\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5643443182790511,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "read and write operations 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
